#!/usr/bin/env python3
"""
Run the KV-cache experiment suite for llama.cpp and Ollama.

This orchestrates a sweep over:
  - engine: ollama vs llamacpp
  - kv_cache_type: f16, q8_0, q4_0
  - context_tokens: 1k -> 32k (exact token prompts generated by generate_prompts.py)

Outputs:
  results/<timestamp>/runs.csv
  results/<timestamp>/traces/<engine>_<kv>_<ctx>.csv
  results/<timestamp>/logs/* (stderr / server logs)
"""

from __future__ import annotations

import os
import sys

# Allow running as: `python experiments/run_suite.py ...`
# (ensure project root is importable so `experiments.*` and `profiling.*` resolve).
_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _ROOT not in sys.path:
    sys.path.insert(0, _ROOT)

import argparse
import csv
import json
from datetime import datetime
from typing import Any, Dict, List, Optional

from experiments.llamacpp_engine import measure_run_with_memory as measure_llamacpp
from experiments.ollama_engine import measure_generate_with_memory, start_server
from profiling.process_sampler import MemSample


def _load_toml(path: str) -> dict:
    import tomllib

    with open(path, "rb") as f:
        return tomllib.load(f)


def _now_stamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def _read_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _write_samples_csv(samples: List[MemSample], path: str, label: str) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", newline="") as f:
        w = csv.DictWriter(
            f,
            fieldnames=["timestamp", "label", "pid", "rss_mb", "vms_mb", "elapsed_s"],
        )
        w.writeheader()
        for s in samples:
            w.writerow(
                {
                    "timestamp": s.timestamp,
                    "label": label,
                    "pid": s.pid,
                    "rss_mb": f"{s.rss_mb:.6f}",
                    "vms_mb": f"{s.vms_mb:.6f}",
                    "elapsed_s": f"{s.elapsed_s:.6f}",
                }
            )


def _load_prompt_manifest(manifest_path: str) -> Dict[int, str]:
    if not os.path.exists(manifest_path):
        raise RuntimeError(
            f"Prompt manifest not found: {manifest_path}\n"
            "Run: python experiments/generate_prompts.py --config experiments/config.toml"
        )
    with open(manifest_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    out: Dict[int, str] = {}
    for p in data.get("prompts", []):
        out[int(p["context_tokens"])] = str(p["path"])
    return out


def main() -> None:
    ap = argparse.ArgumentParser(description="Run KV-cache experiment suite")
    ap.add_argument("--config", default="experiments/config.toml", help="Path to experiments/config.toml")
    args = ap.parse_args()

    cfg = _load_toml(args.config)

    paths = cfg["paths"]
    sweep = cfg["sweep"]
    sampling = cfg["sampling"]

    results_root = paths.get("results_root", "results")
    run_dir = os.path.join(results_root, _now_stamp())
    traces_dir = os.path.join(run_dir, "traces")
    logs_dir = os.path.join(run_dir, "logs")
    os.makedirs(traces_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)

    prompt_map = _load_prompt_manifest(paths["prompt_manifest"])

    engines: List[str] = list(sweep["engine"])
    kv_types: List[str] = list(sweep["kv_cache_type"])
    ctx_list: List[int] = [int(x) for x in sweep["context_tokens"]]
    max_gen = int(sweep["max_gen_tokens"])
    temperature = float(sweep["temperature"])

    interval_s = float(sampling["interval_s"])
    baseline_window_s = float(sampling["baseline_window_s"])

    runs_csv = os.path.join(run_dir, "runs.csv")
    fieldnames = [
        "engine",
        "kv_cache_type",
        "context_tokens",
        "max_gen_tokens",
        "temperature",
        "baseline_idle_rss_mb",
        "peak_prefill_rss_mb",
        "peak_total_rss_mb",
        "prefill_tps",
        "gen_tps",
        "prompt_eval_count",
        "prompt_eval_duration_s",
        "eval_count",
        "eval_duration_s",
        "load_duration_s",
        "total_duration_s",
        "elapsed_wall_s",
        "trace_csv",
        "notes",
    ]

    with open(runs_csv, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        # --- Ollama: restart server per kv type ---
        if "ollama" in engines:
            ollama_cfg = cfg["ollama"]
            base_url = ollama_cfg["base_url"]
            model = ollama_cfg["model"]
            flash_attention = bool(ollama_cfg.get("flash_attention", True))

            warmup = bool(ollama_cfg.get("warmup", True))
            warmup_prompt = str(ollama_cfg.get("warmup_prompt", "Warmup."))
            warmup_max = int(ollama_cfg.get("warmup_max_gen_tokens", 8))

            for kv in kv_types:
                server_log = os.path.join(logs_dir, f"ollama_server_{kv}.log")
                server = start_server(
                    base_url=base_url,
                    kv_cache_type=kv,
                    flash_attention=flash_attention,
                    log_path=server_log,
                )
                try:
                    for ctx in ctx_list:
                        prompt_path = prompt_map.get(ctx)
                        if not prompt_path:
                            raise RuntimeError(f"No prompt file found for ctx={ctx} in manifest")
                        prompt = _read_text(prompt_path)

                        label = f"ollama_{kv}_{ctx}"
                        trace_path = os.path.join(traces_dir, f"{label}.csv")

                        m = measure_generate_with_memory(
                            server=server,
                            label=label,
                            model=model,
                            prompt=prompt,
                            max_gen_tokens=max_gen,
                            temperature=temperature,
                            interval_s=interval_s,
                            baseline_window_s=baseline_window_s,
                            warmup=warmup,
                            warmup_prompt=warmup_prompt,
                            warmup_max_gen_tokens=warmup_max,
                        )

                        # Save trace
                        _write_samples_csv(m["samples"], trace_path, label=label)

                        writer.writerow(
                            {
                                "engine": "ollama",
                                "kv_cache_type": kv,
                                "context_tokens": ctx,
                                "max_gen_tokens": max_gen,
                                "temperature": temperature,
                                "baseline_idle_rss_mb": m.get("baseline_idle_rss_mb"),
                                "peak_prefill_rss_mb": m.get("peak_prefill_rss_mb"),
                                "peak_total_rss_mb": m.get("peak_total_rss_mb"),
                                "prefill_tps": m.get("prefill_tps"),
                                "gen_tps": m.get("gen_tps"),
                                "prompt_eval_count": m.get("prompt_eval_count"),
                                "prompt_eval_duration_s": m.get("prompt_eval_duration_s"),
                                "eval_count": m.get("eval_count"),
                                "eval_duration_s": m.get("eval_duration_s"),
                                "load_duration_s": None,
                                "total_duration_s": m.get("total_duration_s"),
                                "elapsed_wall_s": m.get("elapsed_wall_s"),
                                "trace_csv": trace_path,
                                "notes": f"prompt={prompt_path}; server_log={server_log}",
                            }
                        )
                        f.flush()

                    # After first run on a KV setting, we should not warmup again.
                    warmup = False
                finally:
                    server.stop()

        # --- llama.cpp: run per ctx + kv (process exits each run) ---
        if "llamacpp" in engines:
            llama_cfg = cfg["llamacpp"]
            llama_bin = paths.get("llama_cli", "")
            model_path = paths.get("llamacpp_model", "")
            if not model_path:
                print("ERROR: [paths].llamacpp_model is required for llamacpp runs", file=sys.stderr)
                sys.exit(2)

            ngl = int(llama_cfg.get("ngl", 999))
            margin = int(llama_cfg.get("ctx_margin_tokens", 128))

            for kv in kv_types:
                for ctx in ctx_list:
                    prompt_path = prompt_map.get(ctx)
                    if not prompt_path:
                        raise RuntimeError(f"No prompt file found for ctx={ctx} in manifest")
                    prompt = _read_text(prompt_path)

                    n_ctx_total = int(ctx + max_gen + margin)
                    label = f"llamacpp_{kv}_{ctx}"
                    trace_path = os.path.join(traces_dir, f"{label}.csv")
                    stderr_path = os.path.join(logs_dir, f"{label}.stderr.txt")

                    m = measure_llamacpp(
                        label=label,
                        llama_cli=llama_bin,
                        model_path=model_path,
                        prompt=prompt,
                        n_predict=max_gen,
                        temperature=temperature,
                        n_ctx=n_ctx_total,
                        ngl=ngl,
                        cache_type_k=kv,
                        cache_type_v=kv,
                        interval_s=interval_s,
                        baseline_window_s=baseline_window_s,
                    )

                    _write_samples_csv(m["samples"], trace_path, label=label)
                    with open(stderr_path, "w", encoding="utf-8") as lf:
                        lf.write(m.get("stderr") or "")

                    writer.writerow(
                        {
                            "engine": "llamacpp",
                            "kv_cache_type": kv,
                            "context_tokens": ctx,
                            "max_gen_tokens": max_gen,
                            "temperature": temperature,
                            "baseline_idle_rss_mb": m.get("baseline_idle_rss_mb"),
                            "peak_prefill_rss_mb": m.get("peak_prefill_rss_mb"),
                            "peak_total_rss_mb": m.get("peak_total_rss_mb"),
                            "prefill_tps": m.get("prefill_tps"),
                            "gen_tps": m.get("gen_tps"),
                            "prompt_eval_count": m.get("prompt_eval_count"),
                            "prompt_eval_duration_s": m.get("prompt_eval_duration_s"),
                            "eval_count": m.get("eval_count"),
                            "eval_duration_s": m.get("eval_duration_s"),
                            "load_duration_s": m.get("load_duration_s"),
                            "total_duration_s": m.get("total_duration_s"),
                            "elapsed_wall_s": m.get("elapsed_wall_s"),
                            "trace_csv": trace_path,
                            "notes": f"prompt={prompt_path}; stderr_log={stderr_path}",
                        }
                    )
                    f.flush()

    print(f"Done. Results written to: {run_dir}")


if __name__ == "__main__":
    main()

